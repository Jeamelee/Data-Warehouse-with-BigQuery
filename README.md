Designing and Implementing a Data Warehouse for Nestle Marketing and Distribution

üéØ Project Overview

This project involved the design, implementation, and analysis of a data warehouse solution specifically tailored for the Marketing and Distribution Department of Nestle. The primary goal was to transform raw sales and purchase data into a structured, query-optimized format to support advanced business intelligence (BI) reporting and analytical decision-making.

The project successfully demonstrated proficiency in data warehousing principles, the ETL (Extract, Transform, Load) process, dimensional modeling (Star Schema), and data retrieval using advanced SQL techniques.

Final Project Outcome: 96%

üí° Technologies and Architecture
This project utilized a modern cloud-based approach to data warehousing, focusing on scalability and performance.

| Technology        | Role                                                                 |
|-------------------|----------------------------------------------------------------------|
| Google BigQuery   | Primary cloud data warehouse platform for storage and querying.      |
| SQL               | Used for dimensional modeling, ETL scripting, and BI reporting.     |
| Dimensional Model | Implemented Star Schema (Sales Fact Table + Dimension Tables).       |
| ETL Process       | Extracted, transformed, and loaded raw data into BigQuery warehouse. |


üìê Data Warehouse Schema
The data warehouse was structured using a Star Schema, focusing on sales performance. The key components include:

Fact Table
Sales (Fact): Contains measurable metrics related to transactions. (Note: This was the sole fact table used for the ETL process.)

Dimension Tables
The following dimension tables were created to enable slicing and dicing of the sales data:

-Country
-Customer
-Date
-Payment
-Product
-Salesperson
-Store

üíª Business Intelligence Queries
The final deliverable included several critical SQL queries designed to extract key business insights for the Marketing and Distribution Department. These queries allow users to derive valuable intelligence by analyzing sales data across various dimensions:

| Insight Generated          | Description                                                              |
|----------------------------|--------------------------------------------------------------------------|
| Total Sale by Product      | Provides a clear breakdown of revenue generated by each product line.     |
| Sale Trend Over Time       | Tracks sales performance monthly/annually to identify seasonality.       |
| Top Performing Salesperson | Identifies the most successful salespeople based on total sales value.   |
| Customer Purchase Frequency| Analyzes how often different customer segments make purchases.            |
| Sale by Region             | Compares sales across different geographic regions (e.g., Europe).       |
| Payment Method Analysis    | Reports on sales by payment type (Cash, Credit Card, etc.).              |


‚≠ê My Role and Contribution
I took on the roles of sole Data Analyst and Project Manager for this project, which was technically a two-person group assignment. My responsibilities encompassed the entire lifecycle:

1. Design: Conceptualizing the dimensional model and determining the required fact and dimension tables.

2. Implementation (ETL): Writing and executing the SQL scripts to transform and load the raw data into the BigQuery data warehouse structure.

3. Analysis: Developing and executing advanced SQL queries to generate actionable business intelligence reports.

Collaboration and Guidance:

- Coursera Course: I independently utilized the "Build a Data Warehouse Using BigQuery" course to guide the implementation of best practices for cloud data warehousing. [Course](https://www.coursera.org/learn/build-a-data-warehouse-using-bigquery/home/module/1)

- Lecturer Guidance: I worked closely with my lecturer to receive feedback and ensure the design and implementation met all academic and technical requirements.

üöÄ Getting Started (Simulated)
 This section outlines the general steps required to replicate a similar setup in Google BigQuery.

1. Set up Google Cloud: Ensure you have a Google Cloud project with the BigQuery API enabled.

2. Create Dataset: Create a BigQuery Dataset (e.g., Nestle_DataWarehouse).

3. Load Source Data: Load your raw sales and purchase data into initial staging tables (e.g., Nestle_Sales_and_Purchase).

4. Execute ETL: Run the dimensional modeling SQL scripts (similar to those provided in the project) to create the Fact and Dimension tables in the Nestle_DataWarehouse Dataset.

5. Run Queries: Execute the BI queries to generate reports and gain insights.
